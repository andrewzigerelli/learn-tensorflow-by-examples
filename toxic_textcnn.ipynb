{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/train.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               object\n",
       "comment_text     object\n",
       "toxic             int64\n",
       "severe_toxic      int64\n",
       "obscene           int64\n",
       "threat            int64\n",
       "insult            int64\n",
       "identity_hate     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train.iloc[0,1]\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = train.loc[train.toxic == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8) (15294, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, toxic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "12  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "16  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "42  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6       1             1        1       0       1              0  \n",
       "12      1             0        0       0       0              0  \n",
       "16      1             0        0       0       0              0  \n",
       "42      1             0        1       0       1              1  \n",
       "43      1             0        1       0       1              0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bye! \\n\\nDon't look, come or think of comming back! Tosser.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are gay or antisemmitian? \\n\\nArchangel WHite Tiger\\n\\nMeow! Greetingshhh!\\n\\nUh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\\n\\n1 - If you are anti-semitian, than shave your head bald and go to the skinhead meetings!\\n\\n2 - If you doubt words of the Bible, that homosexuality is a deadly sin, make a pentagram tatoo on your forehead go to the satanistic masses with your gay pals!\\n\\n3 - First and last warning, you fucking gay - I won't appreciate if any more nazi shwain would write in my page! I don't wish to talk to you anymore!\\n\\nBeware of the Dark Side!\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic.iloc[3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN:\n",
    "    \n",
    "    def __init__(self,**params):\n",
    "        # params is a dictionary\n",
    "        self.params = params\n",
    "        self.y = None\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        # X is a list of comments. [[fxxk, ..], [something, ..]]\n",
    "        # len(X) == N == len(y)\n",
    "        # y is a numpy array (N, 6)\n",
    "        self.X = np.array(X)\n",
    "        self.y = y\n",
    "        self._train()\n",
    "    \n",
    "    def predict(self,X):\n",
    "        print('predict')\n",
    "        # X is a list of comments. [[fxxk, ..], [something, ..]]\n",
    "        self.X = X # Xt means X for test data\n",
    "        return self.predictx()\n",
    "    \n",
    "    def predictx(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.params['epochs'] = 1\n",
    "        # build a tf computing graph\n",
    "        logit = self._build()\n",
    "        logit = tf.nn.sigmoid(logit)\n",
    "        preds = []\n",
    "        print('here')\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            self.load(sess)\n",
    "            for c,Xb in enumerate(self._batch_gen(shuffle=False)):\n",
    "                pred = sess.run(logit,feed_dict={self.inputs:Xb})\n",
    "                if c%10 == 0:\n",
    "                    print(\"batch %d predicted\"%(c))\n",
    "                preds.append(pred)\n",
    "        preds = np.vstack(preds)\n",
    "        return preds\n",
    "        \n",
    "    def _train(self):\n",
    "            tf.reset_default_graph()\n",
    "        # build a tf computing graph\n",
    "        logit = self._build()\n",
    "        label = tf.placeholder(dtype=tf.int32,shape=[None,6]) # B,classes\n",
    "        losst = self.get_loss(logit,label)\n",
    "        opt_op = self.get_opt(losst)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            self.load(sess)\n",
    "            for c,(Xb,yb) in enumerate(self._batch_gen(shuffle=True)):\n",
    "                loss,_ = sess.run([losst,opt_op],feed_dict={self.inputs:Xb, label:yb})\n",
    "                if c%10 == 0:\n",
    "                    print(\"batch %d train loss %.4f\"%(c,loss))\n",
    "                if c>100:\n",
    "                    break\n",
    "            self.save(sess)\n",
    "        \n",
    "    def get_opt(self,loss):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        return opt.minimize(loss)\n",
    "        \n",
    "    def get_loss(self, logit, label):\n",
    "        # build the loss tensor\n",
    "        label = tf.cast(label,tf.float32)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,labels=label)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    def save(self, sess):\n",
    "        varss = tf.trainable_variables()\n",
    "        weights = {} # var.name => var.value: a numpy array\n",
    "        for var in varss:\n",
    "            val = sess.run(var)\n",
    "            weights[var.name] = val\n",
    "        pickle.dump(weights,open('weight.p','wb'))\n",
    "\n",
    "    def load(self, sess, path = 'weight.p'):\n",
    "        weights = pickle.load(open(path,'rb'))\n",
    "        varss = tf.trainable_variables()\n",
    "        for var in varss:\n",
    "            value = weights[var.name]\n",
    "            assign_op = var.assign(value)\n",
    "            sess.run(assign_op)\n",
    "        \n",
    "    def _build(self):\n",
    "        \n",
    "        # B is the batch size\n",
    "        # S is the sequence length\n",
    "        # E is the embedding size: 16 by default\n",
    "        # V is the vocab size\n",
    "        # embedding space will be: V x E\n",
    "        # self.inputs => Xb\n",
    "        E = self.params.get('embedding_size',16)\n",
    "        V = self.params['vocab_size']\n",
    "        \n",
    "        netname = 'textCNN'\n",
    "        self.inputs = tf.placeholder(dtype=tf.int32,shape=[None,None]) # B,S\n",
    "        \n",
    "        with tf.variable_scope(netname):\n",
    "            # embedding lookup\n",
    "            print(\"Input:\",self.inputs.get_shape().as_list())\n",
    "            net = self.embedding_lookup(E, V, self.inputs,reuse=False) # dim(net) = B,S,E\n",
    "            print(\"after embedding lookup:\",net.get_shape().as_list())\n",
    "            \n",
    "            net1 = self.conv1d(net,name='conv1',ngrams=3,stride=1,cin=E,nfilters=30)\n",
    "            net2 = self.conv1d(net,name='conv2',ngrams=1,stride=1,cin=E,nfilters=10)\n",
    "            net3 = self.conv1d(net,name='conv3',ngrams=5,stride=1,cin=E,nfilters=50)\n",
    "            \n",
    "            print(\"before maxpool net1: {} net2: {} net3: {}\".format(net1.get_shape().as_list(),net2.get_shape().as_list(),net3.get_shape().as_list()))\n",
    "            \n",
    "            net1 = tf.reduce_max(net1,axis=1)\n",
    "            net2 = tf.reduce_max(net2,axis=1)\n",
    "            net3 = tf.reduce_max(net3,axis=1)\n",
    "            \n",
    "            print(\"after maxpool net1: {} net2: {} net3: {}\".format(net1.get_shape().as_list(),net2.get_shape().as_list(),net3.get_shape().as_list()))\n",
    "           \n",
    "            \n",
    "            net  = tf.concat([net1,net2,net3],axis=1)\n",
    "            print(\"after concat:\",net.get_shape().as_list())\n",
    "            #print(\"after conv1d:\",net.get_shape().as_list())\n",
    "            \n",
    "            net = self.fc(net, 'fc', cin=90, cout=6, activation=None)\n",
    "            print(\"after fc:\",net.get_shape().as_list())\n",
    "            return net\n",
    "            \n",
    "    def fc(self, net, name, cin, cout, activation='relu'):\n",
    "        weights = tf.get_variable(name=name,shape=[cin,cout],dtype=tf.float32,\n",
    "            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        net = tf.matmul(net,weights)\n",
    "        if activation == 'relu':\n",
    "            net = tf.nn.relu(net)\n",
    "        return net\n",
    "        \n",
    "    def conv1d(self, net, name, ngrams, stride, cin, nfilters,activation='relu'):\n",
    "        # filters: kernel_size, cin, cout\n",
    "        filters = tf.get_variable(name='%s_filters'%name,shape=[ngrams,cin,nfilters],dtype=tf.float32,\n",
    "            initializer=tf.contrib.layers.xavier_initializer()) # filter variable\n",
    "        net = tf.nn.conv1d(net, filters, stride, padding='VALID')\n",
    "        if activation == 'relu':\n",
    "            net = tf.nn.relu(net)\n",
    "        return net\n",
    "    \n",
    "    def embedding_lookup(self,E,V,x,reuse=False):\n",
    "        # x is the key\n",
    "        with tf.variable_scope('embedding', reuse=reuse):\n",
    "            \n",
    "            # initialize the embedding matrix variable\n",
    "            w = tf.get_variable(name='w',shape=[V-1,E],dtype=tf.float32,initializer=tf.contrib.layers.xavier_initializer()) # embedding matrix\n",
    "            # w.name = 'textCNN/embedding/w:0'\n",
    "            self.em = w\n",
    "            c = tf.zeros([1,E])\n",
    "            w = tf.concat([c,w],axis=0)\n",
    "            \n",
    "            \n",
    "            # embedding lookup\n",
    "            x = tf.cast(x,tf.int32)\n",
    "            x = tf.nn.embedding_lookup(w, x, name='word_vector')  # (N, T, M) or (N, M)\n",
    "            return x\n",
    "    \n",
    "    def _batch_gen(self, shuffle=True):\n",
    "        X,y = self.X, self.y\n",
    "        B = self.params.get('batch_size', 128)\n",
    "        epochs = self.params.get('epochs', 10)\n",
    "        ids = [i for i in range(len(X))]\n",
    "        batches = len(X)//B + 1\n",
    "        print(epochs,batches)\n",
    "        for epoch in range(epochs):\n",
    "            if shuffle:\n",
    "                random.shuffle(ids)\n",
    "            for i in range(batches):                \n",
    "                idx = ids[i*B:(i+1)*B]\n",
    "                Xb = self.padding(X[idx])\n",
    "                if y is not None:\n",
    "                    yield Xb,y[idx]\n",
    "                else:\n",
    "                    yield Xb\n",
    "            if (i+1)*B < len(X):\n",
    "                idx = ids[(i+1)*B:len(X)]\n",
    "                Xb = self.padding(X[idx])\n",
    "                yield Xb\n",
    "    \n",
    "    def padding(self,X):\n",
    "        Xn = []\n",
    "        maxlen = max([len(i) for i in X])\n",
    "        for x in X:\n",
    "            xn = x+[0]*(maxlen-len(x))\n",
    "            assert len(xn)==maxlen\n",
    "            Xn.append(xn)\n",
    "        return np.array(Xn)\n",
    "    \n",
    "    def _predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows done, size of vocab = 237044\n",
      "100000 rows done, size of vocab = 384696\n",
      "150000 rows done, size of vocab = 509860\n"
     ]
    }
   ],
   "source": [
    "# construct a vocabulary as a dictionary: {word: integer}\n",
    "vocab = {'__none__':0}\n",
    "for index, row in train.iterrows():\n",
    "    comment = row['comment_text']\n",
    "    if index >0 and index%50000 ==0:\n",
    "        print('%d rows done, size of vocab = %d'%(index,len(vocab)))\n",
    "    words = comment.split()\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows done\n",
      "100000 rows done\n",
      "150000 rows done\n"
     ]
    }
   ],
   "source": [
    "# map the word to an integer for all the words\n",
    "# build the X\n",
    "X = []\n",
    "for index, row in train.iterrows():\n",
    "    comment = row['comment_text']\n",
    "    if index >0 and index%50000 ==0:\n",
    "        print('%d rows done'%(index))\n",
    "    words = comment.split()\n",
    "    x = []\n",
    "    for word in words:\n",
    "        x.append(vocab.get(word, 0))\n",
    "    X.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows done\n",
      "100000 rows done\n",
      "150000 rows done\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../input/test.csv.zip')\n",
    "Xt = []\n",
    "for index, row in test.iterrows():\n",
    "    comment = row['comment_text']\n",
    "    if index >0 and index%50000 ==0:\n",
    "        print('%d rows done'%(index))\n",
    "    words = comment.split()\n",
    "    x = []\n",
    "    for word in words:\n",
    "        x.append(vocab.get(word, 0))\n",
    "    Xt.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train.columns.values[2:]\n",
    "Xt = np.array(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571 159571 (159571, 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(X),len(y),y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textCNN(vocab_size=len(vocab),embedding_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [None, None]\n",
      "after embedding lookup: [None, None, 4]\n",
      "before maxpool net1: [None, None, 30] net2: [None, None, 10] net3: [None, None, 50]\n",
      "after maxpool net1: [None, 30] net2: [None, 10] net3: [None, 50]\n",
      "after concat: [None, 90]\n",
      "after fc: [None, 6]\n",
      "batch 0 train loss 0.6932\n",
      "batch 10 train loss 0.6878\n",
      "batch 20 train loss 0.6757\n",
      "batch 30 train loss 0.6538\n",
      "batch 40 train loss 0.6171\n",
      "batch 50 train loss 0.5593\n",
      "batch 60 train loss 0.4951\n",
      "batch 70 train loss 0.4168\n",
      "batch 80 train loss 0.3404\n",
      "batch 90 train loss 0.2365\n",
      "batch 100 train loss 0.2042\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [None, None]\n",
      "after embedding lookup: [None, None, 4]\n",
      "before maxpool net1: [None, None, 30] net2: [None, None, 10] net3: [None, None, 50]\n",
      "after maxpool net1: [None, 30] net2: [None, 10] net3: [None, 50]\n",
      "after concat: [None, 90]\n",
      "after fc: [None, 6]\n",
      "batch 0 train loss 0.1849\n",
      "batch 10 train loss 0.1930\n",
      "batch 20 train loss 0.1750\n",
      "batch 30 train loss 0.1424\n",
      "batch 40 train loss 0.1465\n",
      "batch 50 train loss 0.1788\n",
      "batch 60 train loss 0.1234\n",
      "batch 70 train loss 0.1420\n",
      "batch 80 train loss 0.1780\n",
      "batch 90 train loss 0.1153\n",
      "batch 100 train loss 0.1293\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textCNN(vocab_size=len(vocab),embedding_size=4)\n",
    "yp = model.predict(Xt)\n",
    "yp.shape\n",
    "s = pd.DataFrame(yp,columns = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate'])\n",
    "s['id'] = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pickle.load(open('weight.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textCNN/conv1_filters:0 (3, 4, 30)\n",
      "textCNN/conv2_filters:0 (1, 4, 10)\n",
      "textCNN/embedding/w:0 (532299, 4)\n",
      "textCNN/conv3_filters:0 (5, 4, 50)\n",
      "textCNN/fc:0 (90, 6)\n"
     ]
    }
   ],
   "source": [
    "for name,values in weights.items():\n",
    "    print(name,values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "varss = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(varss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
